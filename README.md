# Vision Agent MCP Server

<!-- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Badges â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<!-- Replace all TODOs with real links once available -->

[![npm](https://img.shields.io/npm/v/vision-tools-mcp?label=npm)](https://www.npmjs.com/package/vision-tools-mcp)
![build](https://github.com/landing-ai/vision-agent-mcp/actions/workflows/ci.yml/badge.svg)

> **Beta â€“ v0.1**  
> This project is **early access** and subject to breaking changes until v1.0.


## Vision Agent MCP Server v0.1 - Overview

Modern LLM â€œagentsâ€ call external tools through the **Model Context Protocol (MCP)**.
**Vision Agent MCP** is a lightweight, side-car MCP server that runs locally on STDIN/STDOUT, translating each tool call from an MCP-compatible client (Claude Desktop, Cursor, Cline, etc.) into an authenticated HTTPS request to Landing AIâ€™s Vision Agent REST APIs. The response JSON, plus any images or masks, is streamed back to the model so that you can issue natural-language computer-vision and document-analysis commands from your editor without writing custom REST code or loading an extra SDK.


## ğŸ“¸ Demo

<https://github.com/user-attachments/assets/4c78ebbb-85ac-4f39-865c-e4de27a57e6b>

## ğŸ§° Supported Use Cases (v0.1)

| Capability                    | Description                                                                                                       |
| ----------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| **`agentic-document-analysis`** | Parse PDFs / images to extract text, tables, charts, and diagrams taking into account layouts and other visual cues. Web Version [here](https://va.landing.ai/demo/doc-extraction).|
| **`text-to-object-detection`** | Detect free-form prompts (â€œall traffic lightsâ€) using OWLv2 / CountGD / Florence-2 / Agentic Object Detection (Web Version [here](https://va.landing.ai/demo/agentic-od)); outputs bounding boxes.        |
| **`text-to-instance-segmentation`** | Pixel-perfect masks via Florence-2 + Segment-Anything-v2 (SAM-2).                                              |
| **`activity-recognition`**     | Recognise multiple activities in video with start/end timestamps.                                           |
| **`depth-pro`**                | High-resolution monocular depth estimation for single images.                                                    |

> Run **`npm run generate-tools`** whenever Vision Agent releases new endpoints. The script fetches the latest OpenAPI spec and regenerates the local tool map automatically.


## ğŸ—º Table of Contents
1. [Quick Start](#-quick-start)
2. [Configuration](#-configuration)
3. [Example Prompts](#-example-prompts)
4. [Architecture & Flow](#-architecture--flow)
5. [Developer Guide](#-developer-guide)
6. [Troubleshooting](#-troubleshooting)
7. [Contributing](#-contributing)
8. [Security & Privacy](#-security--privacy)


## ğŸš€ Quick Start

```bash
# 1  Install
npm install -g vision-tools-mcp

# 2  Set your Vision Agent API key
export VISION_AGENT_API_KEY="<YOUR_API_KEY>"

# 3  Configure your MCP client with the following settings:
{
  "mcpServers": {
    "Vision Agent": {
      "command": "npx",
      "args": ["vision-tools-mcp"],
      "env": {
        "VISION_AGENT_API_KEY": "<YOUR_API_KEY>",
        "OUTPUT_DIRECTORY": "/path/to/output/directory",
        "IMAGE_DISPLAY_ENABLED": "true"
      }
    }
  }
}
```

1. Open your MCP-aware client.
2. Download *street.png* (from the assets folder in this directory, or you can choose any test image).
3. Paste the prompt below (or any prompt):

```
Detect all traffic lights in /Users/cmaloney111/Documents/Landing/mcp/vision-agent-mcp/assets/street.png using text-to-object-detection
```

If your client supports inline resources, youâ€™ll see bounding-box overlays; otherwise, the PNG is saved to your output directory, and the chat shows its path.


### Prerequisites

| Software                 | Minimum Version                          |
| ------------------------ | ---------------------------------------- |
| **Node.js**              | 20 (LTS)                                 |
| **Vision Agent account** | Any paid or free tier (needs API key)    |
| **MCP client**           | Claude Desktop / Cursor / Cline / *etc.* |


## âš™ï¸ Configuration

| ENV var                 | Required | Default    | Purpose                                                |
| ----------------------- | -------- | ---------- | ------------------------------------------------------ |
| `VISION_AGENT_API_KEY`  | **Yes**  | â€”          | Landing AI auth token.                                 |
| `OUTPUT_DIRECTORY`      | No       | â€”          | Where rendered images / masks / depth maps are stored. |
| `IMAGE_DISPLAY_ENABLED` | No       | `true`     | `false` âœ skip rendering                               |

### Sample MCP client entry (`.mcp.json` for VS Code / Cursor)

```jsonc
{
  "mcpServers": {
    "Vision Agent": {
      "command": "npx",
      "args": ["vision-tools-mcp"],
      "env": {
        "VISION_AGENT_API_KEY": "912jkefief09jfjkMfoklwOWdp9293jefklwfweLQWO9jfjkMfoklwDK",
        "OUTPUT_DIRECTORY": "/Users/me/documents/mcp/test",
        "IMAGE_DISPLAY_ENABLED": "true"
      }
    }
  }
}
```


## ğŸ’¡ Example Prompts

| Scenario                     | Prompt (after uploading file)                                                             |
| ---------------------------- | ----------------------------------------------------------------------------------------- |
| Invoice extraction           | *â€œExtract vendor, invoice date & total from this PDF using `agentic-document-analysis`.â€* |
| Pedrestrian Recognition      | *â€œLocate every pedestrian in **street.jpg** via `text-to-object-detection`.â€*             |
| Agricultural segmentation    | *â€œSegment all tomatoes in **kitchen.png** with `text-to-instance-segmentation`.â€*         |
| Activity recognition (video) | *â€œIdentify activities occurring in **match.mp4** via `activity-recognition`.â€*            |
| Depth estimation             | *â€œProduce a depth map for **selfie.png** using `depth-pro`.â€*                             |


## ğŸ— Architecture & Flow

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 1. human prompt            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCP-capable client â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Vision Agent MCP â”‚
â”‚  (Cursor, Claude)  â”‚                            â”‚   (this repo)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–²  6. rendered PNG / JSON                     â”‚ 2. JSON tool call
            â”‚                                             â”‚
            â”‚ 5. preview path / data         3. HTTPS     â”‚
            â”‚                                             â–¼
       local disk  â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                Landing AI Vision Agent
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Cloud APIs
                                           4. JSON / media blob
```

1. **Prompt â†’ tool-call**â€ƒThe client converts your natural-language prompt into a structured MCP call.
2. **Validation**â€ƒThe server validates args with Zod schemas derived from the live OpenAPI spec.
3. **Forward**â€ƒAn authenticated Axios request hits the Vision Agent endpoint.
4. **Response**â€ƒJSON + any base64 media are returned.
5. **Visualization**â€ƒIf enabled, masks / boxes / depth maps are rendered to files.
6. **Return to chat**â€ƒThe MCP client receives data + file paths (or inline previews).


## ğŸ§‘â€ğŸ’» Developer Guide

Hereâ€™s how to dive into the code, add new endpoints, or troubleshoot issues.

### ğŸ“‘ Scripts & Commands

| Script                   | Purpose                                                     |
| ------------------------ | ----------------------------------------------------------- |
| `npm run build`          | Compile TypeScript â†’ `build/` (adds executable bit).        |
| `npm run start`          | Build *and* run (`node build/index.js`).                    |
| `npm run typecheck`      | Type-only check (`tsc --noEmit`).                           |
| `npm run generate-tools` | Fetch latest OpenAPI and regenerate `toolDefinitionMap.ts`. |
| `npm run build:all`      | Convenience: `npm run build` + `npm run generate-tools`.    |

> **Pro Tip**: If you modify any files under `src/` or want to pick up new endpoints from Vision Agent, run `npm run build:all` to recompile + regenerate tool definitions.


### ğŸ“‚ Project Layout

```text
vision-agent-mcp/
â”œâ”€â”€ .eslintrc.json              # ESLint config (optional)
â”œâ”€â”€ .gitignore                  # Ignore node_modules, build/, .env, etc.
â”œâ”€â”€ jest.config.js              # Placeholder for future unit tests
â”œâ”€â”€ mcp-va.md                   # Draft docs (incomplete)
â”œâ”€â”€ package.json                # npm metadata, scripts, dependencies
â”œâ”€â”€ package-lock.json           # Lockfile
â”œâ”€â”€ tsconfig.json               # TypeScript compiler config
â”œâ”€â”€ .env                        # Your environment variables (not committed)
â”‚
â”œâ”€â”€ src/                        # TypeScript source code
â”‚   â”œâ”€â”€ generateTools.ts        # Dev script: fetch OpenAPI â†’ generate MCP tool definitions (Zod schemas)
â”‚   â”œâ”€â”€ index.ts                # Entry point: load .env, start MCP server, handle signals
â”‚   â”œâ”€â”€ toolDefinitionMap.ts    # Auto-generated MCP tool definitions (donâ€™t edit by hand)
â”‚   â”œâ”€â”€ toolUtils.ts            # Helpers to build MCP tool objects (metadata, descriptions)
â”‚   â”œâ”€â”€ types.ts                # Core TS interfaces (MCP, environment config, etc.)
â”‚   â”‚
â”‚   â”œâ”€â”€ server/                 # MCP server logic
â”‚   â”‚   â”œâ”€â”€ index.ts            # Create & start the MCP server (Server + Stdio transport)
â”‚   â”‚   â”œâ”€â”€ handlers.ts         # `handleListTools` & `handleCallTool` implementations
â”‚   â”‚   â”œâ”€â”€ visualization.ts    # Post-process & save image/video outputs (masks, boxes, depth maps)
â”‚   â”‚   â””â”€â”€ config.ts           # Load & validate .env, export SERVER_CONFIG & EnvConfig
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                  # Generic utilities
â”‚   â”‚   â”œâ”€â”€ file.ts             # File handling (base64 encode images/PDFs, read streams)
â”‚   â”‚   â””â”€â”€ http.ts             # Axios wrappers & error formatting
â”‚   â”‚
â”‚   â””â”€â”€ validation/             # Zod schema generation & argument validation
â”‚       â””â”€â”€ schema.ts           # Convert JSON Schema â†’ Zod, validate incoming tool args
â”‚
â”œâ”€â”€ build/                      # Compiled JavaScript (generated after `npm run build`)
â”‚   â”œâ”€â”€ index.js
â”‚   â”œâ”€â”€ generateTools.js
â”‚   â”œâ”€â”€ toolDefinitionMap.js
â”‚   â””â”€â”€ â€¦                       # Mirror of `src/` structure
â”‚
â”œâ”€â”€ output/                     # Runtime artifacts (bounding boxes, masks, depth maps, etc.)
â”‚
â””â”€â”€ assets/                     # Static assets (e.g., demo.gif)
    â””â”€â”€ demo.gif
```

### ğŸ” Key Components

1. **`src/generateTools.ts`**

   * Fetches `https://api.va.landing.ai/openapi.json` (Vision Agentâ€™s public OpenAPI).
   * Filters endpoints via a whitelist (or you can disable filtering to include all).
   * Converts JSON Schema â†’ Zod schemas, writes `toolDefinitionMap.ts` with a `Map<string, McpToolDefinition>`.
   * Run: `npm run generate-tools`.

2. **`src/toolDefinitionMap.ts`**

   * Contains a map of tool names â†’ MCP definitions (name, description, inputSchema, endpoint, HTTP method).
   * Generated automaticallyâ€”**do NOT edit by hand**.

3. **`src/server/handlers.ts`**

   * Implements `handleListTools`: returns `[ { name, description, inputSchema } ]`.
   * Implements `handleCallTool`:

     * Validates incoming `arguments` with Zod.
     * If file-based args (e.g., `imagePath`, `pdfPath`), reads & base64-encodes via `src/utils/file.ts`.
     * Builds a multipart/form-data or JSON payload for Axios.
     * Calls Vision Agent endpoint, catches errors, returns MCP-compliant JSON response.
     * If `IMAGE_DISPLAY_ENABLED=true`, calls `src/server/visualization.ts` to save PNGs/JSON.

4. **`src/server/visualization.ts`**

   * Post-processes masks (base64 â†’ PNG).
   * Optionally overlays bounding boxes or segmentation masks on the original image, saves to `OUTPUT_DIRECTORY`.
   * Returns file paths in MCP result so your client can render them.

5. **`src/utils/file.ts`**

   * `readFileAsBase64(path: string): Promise<string>`: Reads any binary (image, PDF, video) and returns base64.
   * `loadFileStream(path: string)`: Returns a Node.js stream for large file uploads.

6. **`src/utils/http.ts`**

   * Configures Axios with base URL `https://api.va.landing.ai`.
   * Adds `Authorization: Bearer ${VISION_AGENT_API_KEY}` header.
   * Wraps calls to Vision Agent endpoints, handles 4xx/5xx, formats errors into MCP error objects.

7. **`src/validation/schema.ts`**

   * Contains helpers to convert JSON Schema (from OpenAPI) â†’ Zod.
   * Exposes a function `buildZodSchema(jsonSchema: any): ZodObject` used by `generateTools.ts`.

8. **`src/index.ts`**

   * Loads `dotenv` (reads `.env`).
   * Validates required env vars (`VISION_AGENT_API_KEY`).
   * Imports generated `toolDefinitionMap`.
   * Creates an MCP `Server` (from `@modelcontextprotocol/sdk/server`) with `StdioServerTransport`.
   * Wires `ListTools` â†’ `handleListTools`, `CallTool` â†’ `handleCallTool`.
   * Logs startup info:

     ```
     vision-tools-api MCP Server (v0.1.0) running on stdio, proxying to https://api.va.landing.ai
     ```
   * Listens for `SIGINT`/`SIGTERM` to gracefully shut down.


### ğŸš§ Error Handling & Logs

* **Validation Errors**
  If you send invalid or missing parameters, the server returns:

  ```json
  {
    "id": 3,
    "error": {
      "code": -32602,
      "message": "Validation error: missing required parameter â€˜imagePathâ€™"
    }
  }
  ```
* **Network Errors**
  Axios errors (timeouts, 5xx) are caught and returned as:

  ```json
  {
    "id": 4,
    "error": {
      "code": -32000,
      "message": "Vision Agent API error: 502 Bad Gateway"
    }
  }
  ```
* **Internal Exceptions**
  Uncaught exceptions in handlers produce:

  ```json
  {
    "id": 5,
    "error": {
      "code": -32603,
      "message": "Internal error: Unexpected token in JSON at position 345"
    }
  }
  ```


## ğŸ›Ÿ Troubleshooting

<details>
<summary><strong>Authentication failed</strong></summary>

* Verify `VISION_AGENT_API_KEY` is correct and active.
* Free tiers have rate limitsâ€”check your dashboard.
* Ensure outbound HTTPS to `api.va.landing.ai` isnâ€™t blocked by a proxy/VPN.

</details>

<details>
<summary><strong>â€œTool not foundâ€ in chat</strong></summary>

The local tool map may be stale. Run:

```bash
npm run generate-tools
npm start
```

</details>

<details>
<summary><strong>Node &lt; 20 error</strong></summary>

The code uses the Blob & FormData APIs natively introduced in Node 20.
Upgrade via `nvm install 20` (mac/Linux) or download from nodejs.org if on Windows.

</details>


## ğŸ¤ Contributing

We love PRs!

1. **Fork** â†’ `git checkout -b feature/my-feature`.
2. `npm run typecheck` (no errors)
3. Open a PR explaining **what** and **why**.

## ğŸ”’ Security & Privacy

* The MCP server runs **locally**, so no files are forwarded anywhere except Landing AIâ€™s API endpoints you explicitly call.
* Output images/masks are written to `OUTPUT_DIRECTORY` **only on your machine**.
* No telemetry is collected by this project.


> *Made with â¤ï¸ by the LandingAI Team.*

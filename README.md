# Vision Agent MCP Server

<!-- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Badges â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
<!-- Replace all TODOs with real links once available -->

[![npm](https://img.shields.io/npm/v/vision-tools-mcp?label=npm)](https://www.npmjs.com/package/vision-tools-mcp)
![build](https://github.com/landing-ai/vision-agent-mcp/actions/workflows/ci.yml/badge.svg)

> **Beta â€“ v0.1**  
> This project is **early access** and subject to breaking changes until v1.0.


## Vision Agent MCP Server v0.1 - Overview

Modern LLM â€œagentsâ€ call external tools through the **Model Context Protocol (MCP)**.
**Vision Agent MCP** is a lightweight, side-car MCP server that runs locally on STDIN/STDOUT, translating each tool call from an MCP-compatible client (Claude Desktop, Cursor, Cline, etc.) into an authenticated HTTPS request to Landing AIâ€™s Vision Agent REST APIs. The response JSON, plus any images or masks, is streamed back to the model so that you can issue natural-language computer-vision and document-analysis commands from your editor without writing custom REST code or loading an extra SDK.


## ğŸ“¸ Demo

![Demo of Vision Agent + Claude Code](assets/demo.gif)


## ğŸ§° Supported Use Cases (v0.1)

| Capability                    | Description                                                                                                       |
| ----------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| **`agentic-document-analysis`** | Parse PDFs / images to extract text, tables, charts, and diagrams taking into account layouts and other visual cues. Web Version [here](https://va.landing.ai/demo/doc-extraction).|
| **`text-to-object-detection`** | Detect free-form prompts (â€œall traffic lightsâ€) using OWLv2 / CountGD / Florence-2 / Agentic Object Detection (Web Version [here](https://va.landing.ai/demo/agentic-od)); outputs bounding boxes.        |
| **`text-to-instance-segmentation`** | Pixel-perfect masks via Florence-2 + Segment-Anything-v2 (SAM-2).                                              |
| **`activity-recognition`**     | Recognise multiple activities in video with start/end timestamps.                                           |
| **`depth-pro`**                | High-resolution monocular depth estimation for single images.                                                    |

> Run **`npm run generate-tools`** whenever Vision Agent releases new endpoints. The script fetches the latest OpenAPI spec and regenerates the local tool map automatically.


## ğŸ—º Table of Contents
1. [Quick Start](#-quick-start)
2. [Configuration](#-configuration)
3. [Example Prompts](#-example-prompts)
4. [Architecture & Flow](#-architecture--flow)
5. [Developer Guide](#-developer-guide)
6. [Troubleshooting](#-troubleshooting)
7. [Contributing](#-contributing)
8. [Security & Privacy](#-security--privacy)


## ğŸš€ Quick Start

```bash
# 1  Install
npm install -g vision-tools-mcp

# 2  Set your Vision Agent API key
export VISION_AGENT_API_KEY="<YOUR_API_KEY>"

# 3  Configure your MCP client with the following settings:
{
  "mcpServers": {
    "Vision Agent": {
      "command": "npx",
      "args": ["vision-tools-mcp"],
      "env": {
        "VISION_AGENT_API_KEY": "<YOUR_API_KEY>",
        "OUTPUT_DIRECTORY": "/path/to/output/directory",
        "IMAGE_DISPLAY_ENABLED": "true"
      }
    }
  }
}
```

1. Open your MCP-aware client.
2. Download *street.png* (from the assets folder in this directory, or you can choose any test image).
3. Paste the prompt below (or any prompt):

```
Detect all traffic lights in /Users/cmaloney111/Documents/Landing/mcp/vision-agent-mcp/assets/street.png using text-to-object-detection
```

If your client supports inline resources, youâ€™ll see bounding-box overlays; otherwise, the PNG is saved to your output directory, and the chat shows its path.


### Prerequisites

| Software                 | Minimum Version                          |
| ------------------------ | ---------------------------------------- |
| **Node.js**              | 20 (LTS)                                 |
| **Vision Agent account** | Any paid or free tier (needs API key)    |
| **MCP client**           | Claude Desktop / Cursor / Cline / *etc.* |


## âš™ï¸ Configuration

| ENV var                 | Required | Default    | Purpose                                                |
| ----------------------- | -------- | ---------- | ------------------------------------------------------ |
| `VISION_AGENT_API_KEY`  | **Yes**  | â€”          | Landing AI auth token.                                 |
| `OUTPUT_DIRECTORY`      | No       | -          | Where rendered images / masks / depth maps are stored. |
| `IMAGE_DISPLAY_ENABLED` | No       | `true`     | `false` âœ skip rendering                               |

### Sample MCP client entry (`.mcp.json` for VS Code / Cursor)

```jsonc
{
  "mcpServers": {
    "Vision Agent": {
      "command": "npx",
      "args": ["vision-tools-mcp"],
      "env": {
        "VISION_AGENT_API_KEY": "912jkefief09jfjkMfoklwOWdp9293jefklwfweLQWO9jfjkMfoklwDK",
        "OUTPUT_DIRECTORY": "/Users/me/documents/mcp/test",
        "IMAGE_DISPLAY_ENABLED": "true"
      }
    }
  }
}
```


## ğŸ’¡ Example Prompts

| Scenario                     | Prompt (after uploading file)                                                             |
| ---------------------------- | ----------------------------------------------------------------------------------------- |
| Invoice extraction           | *â€œExtract vendor, invoice date & total from this PDF using `agentic-document-analysis`.â€* |
| Pedrestrian Recognition      | *â€œLocate every pedestrian in **street.jpg** via `text-to-object-detection`.â€*             |
| Agricultural segmentation    | *â€œSegment all tomatoes in **kitchen.png** with `text-to-instance-segmentation`.â€*         |
| Activity recognition (video) | *â€œIdentify activities occurring in **match.mp4** via `activity-recognition`.â€*            |
| Depth estimation             | *â€œProduce a depth map for **selfie.png** using `depth-pro`.â€*                             |


## ğŸ— Architecture & Flow

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 1. human prompt            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCP-capable client â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Vision Agent MCP â”‚
â”‚  (Cursor, Claude)  â”‚                            â”‚   (this repo)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–²  6. rendered PNG / JSON                     â”‚ 2. JSON tool call
            â”‚                                             â”‚
            â”‚ 5. preview path / data         3. HTTPS     â”‚
            â”‚                                             â–¼
       local disk  â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                Landing AI Vision Agent
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Cloud APIs
                                           4. JSON / media blob
```

1. **Prompt â†’ tool-call**â€ƒThe client converts your natural-language prompt into a structured MCP call.
2. **Validation**â€ƒThe server validates args with Zod schemas derived from the live OpenAPI spec.
3. **Forward**â€ƒAn authenticated Axios request hits the Vision Agent endpoint.
4. **Response**â€ƒJSON + any base64 media are returned.
5. **Visualization**â€ƒIf enabled, masks / boxes / depth maps are rendered to files.
6. **Return to chat**â€ƒThe MCP client receives data + file paths (or inline previews).


## ğŸ§‘â€ğŸ’» Developer Guide

| Script                   | Purpose                                                            |
| ------------------------ | ------------------------------------------------------------------ |
| `npm run build`          | Compile TS â†’ `build/` (adds executable bit).                       |
| `npm run start`          | Build *and* run (`node build/index.js`).                           |
| `npm run typecheck`      | Type-only check (`tsc --noEmit`).                                  |
| `npm run generate-tools` | Fetch latest public OpenAPI and regenerate `toolDefinitionMap.ts`. |
| `npm run build:all`      | Convenience: `build` + `generate-tools`.                           |

### Project layout

```text
src/
  index.ts            # CLI entry (dotenv, signal handlers, startServer)
  server/
    index.ts          # MCP server w/ Stdio transport
    handlers.ts       # listTools, callTool, etc.
    visualization.ts  # camera-ready rendering for each tool
  generateTools.ts    # Dev script (OpenAPI â†’ TS map)
  utils/              # file.ts, http.ts, etc.
build/                # compiled JS (git-ignored)
output/               # runtime artifacts (bounding boxes, masks, â€¦)
```


## ğŸ›Ÿ Troubleshooting

<details>
<summary><strong>Authentication failed</strong></summary>

* Verify `VISION_AGENT_API_KEY` is correct and active.
* Free tiers have rate limitsâ€”check your dashboard.
* Ensure outbound HTTPS to `api.va.landing.ai` isnâ€™t blocked by a proxy/VPN.

</details>

<details>
<summary><strong>â€œTool not foundâ€ in chat</strong></summary>

The local tool map may be stale. Run:

```bash
npm run generate-tools
npm start
```

</details>

<details>
<summary><strong>Node &lt; 20 error</strong></summary>

The code uses the Blob & FormData APIs natively introduced in Node 20.
Upgrade via `nvm install 20` (mac/Linux) or download from nodejs.org if on Windows.

</details>


## ğŸ¤ Contributing

We love PRs!

1. **Fork** â†’ `git checkout -b feature/my-feature`.
2. `npm run typecheck` (no errors)
3. Open a PR explaining **what** and **why**.

## ğŸ”’ Security & Privacy

* The MCP server runs **locally**, so no files are forwarded anywhere except Landing AIâ€™s API endpoints you explicitly call.
* Output images/masks are written to `OUTPUT_DIRECTORY` **only on your machine**.
* No telemetry is collected by this project.


> *Made with â¤ï¸ by the LandingAI Team.*
